{"name":"San Francisco Crime Classification (Kaggle competition) using Spark and Logistic Regression","tagline":"","body":"# San Francisco Crime Classification (Kaggle competition) using Spark and Logistic Regression\r\n\r\n## Overview\r\n***\r\n\r\nThe \"San Francisco Crime Classification\" challenge, is a [Kaggle](https://www.kaggle.com) competition aimed to predict the category of the crimes that occurred in the city, given the time and location of the incident.\r\n\r\nIn this post, I explain and outline my second solution to this challenge. This time\r\nusing Spark and Python.\r\n\r\nLink to the competition: [San Francisco Crime Classification](https://www.kaggle.com/c/sf-crime)\r\n\r\n## Learning method\r\n***\r\n\r\nThe algorithm chosen for the implemented solution, is a [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression), a classification model based on regression where the dependent variable (what we want to predict) is categorical (opposite of continuous).\r\n\r\n## Data\r\n***\r\nThe competition provides two dataset: a train data set and a test dataset. The\r\ntrain dataset is made of 878049 observations and the test dataset, of 884262\r\nobservations.\r\n\r\nBoth of them contains incidents from January 1, 2003 to May 13, 2015.\r\n\r\n### Data fields\r\n* Dates : timestamp of the crime incident.\r\n* Category: Category of the incident. Also, this is the variable we want to predict.\r\nThis variable is available only in the train dataset.\r\n* Descript: A short description of the incident. This variable is available only \r\nin the train dataset.\r\n* DayOfWeek: Day of the week where the incident occurred.\r\n* PdDistrict: Police Department District\r\n* Resolution: Outcome of the incident. This variable is available only in the \r\ntrain dataset.\r\n* Address: Address of the incident.\r\n* X: Longitude\r\n* Y: Latitude\r\n\r\n## Model development\r\n***\r\n\r\n#### Setting up Spark\r\n\r\nThe first lines of the script are for setting the configuration of Spark. The `setMaster(...)` parameter accept the URL of the master. Usually, I check for this URL on the Spark Web UI, available at `http://localhost:8080`. The second parameter is the name of the app, and the last one is the amount of memory per worker, which I set to 2 GB.\r\n\r\n```python\r\nconf = SparkConf().setMaster(\"spark://spark.master.url:7077\").setAppName(\r\n    \"SFCrime-Kaggle\"). \\\r\n    set(\"spark.executor.memory\", \"2g\")\r\nsc = SparkContext(conf=conf)\r\nsqlContext = SQLContext(sc)4w\r\n```\r\n#### Loading data and required packages\r\n\r\nFor loading the train and test dataset (both csv files), I used the package [spark-csv][http://spark-packages.org/package/databricks/spark-csv].\r\n\r\nTo download the package and to add it to the project, use this command `$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.11:1.2.0-s_2.11` when using spark-shell, spark-submit or pyspark.\r\n\r\nNow, we load both files, and use `registerTempTable` on the train dataset, to run SQL statements on it.\r\n\r\n```python\r\n# Import both the train and test dataset and register them as tables\r\ntrain = sqlContext.read.format('com.databricks.spark.csv').options(\r\n    header='true') \\\r\n    .load('train.csv')\r\ntrain.registerTempTable('train')\r\n\r\ntest = sqlContext.read.format('com.databricks.spark.csv').options(\r\n    header='true') \\\r\n    .load('test.csv')\r\n```\r\n\r\n#### Preparing the dataset\r\n\r\nAfter loading the datasets, my next step was to do some make some modifications on the training dataset and preparing it for the training phase.\r\n\r\n```python\r\n# Get all the unique categories and add them to a dictionary\r\ncrimeCategories = sqlContext.sql(\r\n    'SELECT DISTINCT Category FROM train').collect()\r\ncrimeCategories.sort()\r\n\r\ncategories = {}\r\nfor category in crimeCategories:\r\n    categories[category.Category] = float(len(categories))\r\n```\r\n\r\nThe first step was to get all the unique crime categories and add them to a dictionary, using the category as the key and an integer (the current size of the dictionary as the time of insertion), as a value. So, instead of using the category string as the response (what we want to predict), we are using an integer.\r\n...\r\n\r\nThen, I created a `HashingTF` object which does a similar job do what I did with the dictionary; HashingTF maps a sequence of terms into an integer. We will use this object to convert the vectors of predictors into a vector of numeric values.\r\n\r\n```python\r\n# HashingTF transforms a string into a numerical value\t\r\nhtf = HashingTF(5000)\r\n```\r\n\r\nNow, we use Spark's `map` function to convert every observation of the dataset into a `LabeledPoint`, an object that contains a label and a vector (either sparse or dense) and that is used for classification in Spark.\r\n\r\nThe features or predictors that I used for the model are the day of week when the incident occurred, the police district where it occurred and the hour.\r\n\r\n```python\r\n# Create LabeledPoint object\r\ntrainingData = train.map(lambda x: LabeledPoint(categories[x.Category],\r\n                                                htf.transform(\r\n                                                    [x.DayOfWeek, x.PdDistrict,\r\n                                                     datetime.strptime(x.Dates,\r\n                                                                       '%Y-%m-%d %H:%M:%S').hour])))\r\n```\r\n\r\n#### Train the model and predict.\r\n\r\nAfter pre-processing the data, the next step is to train the model.\r\n\r\n```python\r\n# Train the model\r\nlogisticRegressionModel = LogisticRegressionWithLBFGS.train(trainingData,\r\n                                                            iterations=100,\r\n                                                            numClasses=39)\r\n```\r\nNote that this time I didn't checked the training error.\r\n\r\nThis step is follow by preparing the test dataset in a similar way as the training one.\r\n\r\n```python\r\n# Prepare the testting dataset\r\ntestingSet = test.map(lambda x: htf.transform([x.DayOfWeek, x.PdDistrict,\r\n                                               datetime.strptime(x.Dates,\r\n                                                                 '%Y-%m-%d %H:%M:%S').hour]))\r\n```\r\n\r\nAnd finally, we predict and save the result.\r\n\r\n```python\r\n# Predict using the day of the week and the police district and hour of crime\r\npredictions = logisticRegressionModel.predict(testingSet)\r\npredictions.saveAsTextFile('predictionsSpark')\r\n```\r\n\r\n## Conclusion\r\n***\r\n\r\nThe score received this time, was a bit lower than my first attempt (26.78360 and 26.74064). Before I started working with this algorithm, my original plan was to calculate the predicted probability for each class, However, in Spark, this can be done in a binary classification problem and since in this problem, there are 39 possible outcomes, it wasn't going to work. So, since I had already written most of the code, I decided to continue forward and finish it.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}