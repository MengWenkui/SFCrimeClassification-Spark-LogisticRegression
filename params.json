{"name":"San Francisco Crime Classification (Kaggle competition) using Spark and Logistic Regression","tagline":"","body":"# San Francisco Crime Classification (Kaggle competition) using Spark and Logistic Regression\r\n\r\n## Overview\r\n***\r\n\r\nThe \"San Francisco Crime Classification\" challenge, is a [Kaggle](https://www.kaggle.com) competition aimed to predict the category of the crimes that occurred in the city, given the time and location of the incident.\r\n\r\nIn this post, I explain and outline my second solution to this challenge. This time\r\nusing Spark and Python.\r\n\r\nLink to the competition: [San Francisco Crime Classification](https://www.kaggle.com/c/sf-crime)\r\n\r\n## Learning method\r\n***\r\n\r\nThe algorithm chosen for the implemented solution, is a [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression), a classification model based on regression where the dependent variable (what we want to predict) is categorical (opposite of continuous).\r\n\r\n## Data\r\n***\r\nThe competition provides two dataset: a train data set and a test dataset. The\r\ntrain dataset is made of 878049 observations and the test dataset, of 884262\r\nobservations.\r\n\r\nBoth of them contains incidents from January 1, 2003 to May 13, 2015.\r\n\r\n### Data fields\r\n* Dates : timestamp of the crime incident.\r\n* Category: Category of the incident. Also, this is the variable we want to predict.\r\nThis variable is available only in the train dataset.\r\n* Descript: A short description of the incident. This variable is available only \r\nin the train dataset.\r\n* DayOfWeek: Day of the week where the incident occurred.\r\n* PdDistrict: Police Department District\r\n* Resolution: Outcome of the incident. This variable is available only in the \r\ntrain dataset.\r\n* Address: Address of the incident.\r\n* X: Longitude\r\n* Y: Latitude\r\n\r\n## Model development\r\n\r\n#### Setting up Spark\r\n\r\nThe first lines of the script are for setting the configuration of Spark. The `setMaster(...)` parameter accept the URL of the master. Usually, I check for this URL on the Spark Web UI, available at `http://localhost:8080`. The second parameter is the name of the app, and the last one is the amount of memory per worker, which I set to 2 GB.\r\n\r\n```python\r\nconf = SparkConf().setMaster(\"spark://spark.master.url:7077\").setAppName(\r\n    \"SFCrime-Kaggle\"). \\\r\n    set(\"spark.executor.memory\", \"2g\")\r\nsc = SparkContext(conf=conf)\r\nsqlContext = SQLContext(sc)4w\r\n```\r\n#### Loading data and required packages\r\n\r\nFor loading the train and test dataset (both csv files), I used the package [spark-csv][http://spark-packages.org/package/databricks/spark-csv].\r\n\r\nTo download the package and to add it to the project, use this command `$SPARK_HOME/bin/spark-shell --packages com.databricks:spark-csv_2.11:1.2.0-s_2.11` when using spark-shell, spark-submit or pyspark.\r\n\r\nNow, we load both files, and use `registerTempTable` on the train dataset, to run SQL statements on it.\r\n\r\n```python\r\n# Import both the train and test dataset and register them as tables\r\ntrain = sqlContext.read.format('com.databricks.spark.csv').options(\r\n    header='true') \\\r\n    .load('train.csv')\r\ntrain.registerTempTable('train')\r\n\r\ntest = sqlContext.read.format('com.databricks.spark.csv').options(\r\n    header='true') \\\r\n    .load('test.csv')\r\n```\r\n\r\n#### Preparing the dataset\r\n\r\nAfter loading the datasets, my next step was to do some make some modifications on the training dataset and preparing it for the training phase.\r\n\r\n```python\r\n# Get all the unique categories and add them to a dictionary\r\ncrimeCategories = sqlContext.sql(\r\n    'SELECT DISTINCT Category FROM train').collect()\r\ncrimeCategories.sort()\r\n\r\ncategories = {}\r\nfor category in crimeCategories:\r\n    categories[category.Category] = float(len(categories))\r\n```\r\n\r\nThe first step was to get all the unique crime categories and add them to a dictionary, using the category as the key and an integer (the current size of the dictionary as the time of insertion), as a value. So, instead of using the category string as the response (what we want to predict), we are using an integer.\r\n...\r\n\r\nThen, I created a `HashingTF` object which does a similar job do what I did with the dictionary. HashingTF maps a sequence of terms into an integer.","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}