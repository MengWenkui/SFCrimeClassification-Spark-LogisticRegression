{"name":"San Francisco Crime Classification (Kaggle competition) using Spark and Logistic Regression","tagline":"","body":"# San Francisco Crime Classification (Kaggle competition) using Spark and Logistic Regression\r\n\r\n## Overview\r\n***\r\n\r\nThe \"San Francisco Crime Classification\" challenge, is a [Kaggle](https://www.kaggle.com) competition aimed to predict the category of the crimes that occurred in the city, given the time and location of the incident.\r\n\r\nIn this post, I explain and outline my second solution to this challenge. This time\r\nusing Spark and Python.\r\n\r\nLink to the competition: [San Francisco Crime Classification](https://www.kaggle.com/c/sf-crime)\r\n\r\n## Learning method\r\n***\r\n\r\nThe algorithm chosen for the implemented solution, is a [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression), a classification model based on regression where the dependent variable (what we want to predict) is categorical (opposite of continuous).\r\n\r\n## Data\r\n***\r\nThe competition provides two dataset: a train data set and a test dataset. The\r\ntrain dataset is made of 878049 observations and the test dataset, of 884262\r\nobservations.\r\n\r\nBoth of them contains incidents from January 1, 2003 to May 13, 2015.\r\n\r\n### Data fields\r\n* Dates : timestamp of the crime incident.\r\n* Category: Category of the incident. Also, this is the variable we want to predict.\r\nThis variable is available only in the train dataset.\r\n* Descript: A short description of the incident. This variable is available only \r\nin the train dataset.\r\n* DayOfWeek: Day of the week where the incident occurred.\r\n* PdDistrict: Police Department District\r\n* Resolution: Outcome of the incident. This variable is available only in the \r\ntrain dataset.\r\n* Address: Address of the incident.\r\n* X: Longitude\r\n* Y: Latitude\r\n\r\n## Model development\r\n\r\n#### Setting up Spark\r\n\r\nThe first lines of the script are for setting the configuration of Spark. The `setMaster(...)` parameter accept the URL of the master. Usually, I check for this URL on the Spark Web UI, available at `localhost:8080`. The second parameter is the name of the app, and the last one is the amount of memory per worker, which I set to 2 GB.\r\n\r\n```python\r\nconf = SparkConf().setMaster(\"spark://spark.master.url:7077\").setAppName(\r\n    \"SFCrime-Kaggle\"). \\\r\n    set(\"spark.executor.memory\", \"2g\")\r\nsc = SparkContext(conf=conf)\r\nsqlContext = SQLContext(sc)4w\r\n```\r\n#### Loading data and required packages\r\n\r\n\r\n...","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}